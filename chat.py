import os
import json
import numpy as np
import faiss
from openai import OpenAI
from typing import List, Dict

# ------------------- CONFIGURACI√ìN NECESARIA -------------------
CARPETA_RAG = "./mi_rag"  # Carpeta donde se guard√≥ Faiss y los chunks

# Configuraci√≥n de la API de LM Studio (Puerto por defecto 1234)
LM_STUDIO_URL = "http://localhost:1234/v1" 

# Modelos (DEBEN estar corriendo como servidores en LM Studio)
NOMBRE_EMBEDDING_MODELO = "text-embedding-nomic-embed-text-v2-moe" 
NOMBRE_CHAT_MODELO = "openai/gpt-oss-20b"
# ---------------------------------------------------------------

# Conexi√≥n a la API de LM Studio
try:
    cliente_api = OpenAI(base_url=LM_STUDIO_URL, api_key="lm-studio")
except Exception as e:
    print(f"‚ùå ERROR: No se pudo crear el cliente de OpenAI. ¬øEst√° el servidor LM Studio corriendo? {e}")
    exit()

# ----------------------------------------
## 1. Carga del √çndice y del Contexto
# ----------------------------------------
def cargar_base_de_datos(carpeta: str):
    """Carga el √≠ndice FAISS y los chunks de texto."""
    print("‚è≥ Cargando √≠ndice FAISS y chunks...")
    try:
        index = faiss.read_index(os.path.join(carpeta, "faiss.index"))
        with open(os.path.join(carpeta, "chunks.json"), "r", encoding="utf-8") as f:
            metadatos = json.load(f)
        print("‚úÖ Base de datos cargada correctamente.")
        return index, metadatos
    except FileNotFoundError as e:
        print(f"‚ùå ERROR: Archivos FAISS o JSON no encontrados en {carpeta}. Ejecuta primero el script de indexaci√≥n.")
        raise e

index, metadatos = cargar_base_de_datos(CARPETA_RAG)

# ----------------------------------------
## 2. Funciones de B√∫squeda y Generaci√≥n
# ----------------------------------------

def vectorizar_pregunta(pregunta: str) -> np.ndarray:
    """Convierte la pregunta en un vector usando el modelo Nomic (via LM Studio)."""
    try:
        # Pide a la API de LM Studio que vectorice la pregunta
        respuesta = cliente_api.embeddings.create(
            model=NOMBRE_EMBEDDING_MODELO,
            input=[pregunta]
        )
        # Retorna el primer y √∫nico embedding
        return np.array(respuesta.data[0].embedding, dtype='float32')
    except Exception as e:
        print(f"‚ùå ERROR al vectorizar la pregunta: {e}")
        return None

def buscar_contexto(vector_pregunta: np.ndarray, index: faiss.Index, metadatos: List[Dict], k: int = 4) -> str:
    """Busca los K chunks m√°s relevantes en FAISS y forma el contexto."""
    
    # 1. B√∫squeda FAISS: Distancias (D) e √çndices (I)
    vector_pregunta = vector_pregunta.reshape(1, -1) # Formato requerido por Faiss
    D, I = index.search(vector_pregunta, k) 

    contextos_recuperados = []
    fuentes_usadas = set()

    # 2. Recuperar el texto original (chunks)
    for idx in I[0]:
        if idx >= 0 and idx < len(metadatos):
            chunk = metadatos[idx]
            contextos_recuperados.append(f"--- Fuente: {chunk['fuente']} ---\n{chunk['texto']}")
            fuentes_usadas.add(chunk['fuente'])
    
    # Unir todos los chunks relevantes en un solo string de contexto
    contexto_str = "\n\n".join(contextos_recuperados)
    
    print(f"\nüîç Contexto recuperado de {len(contextos_recuperados)} fragmentos. Fuentes √∫nicas: {list(fuentes_usadas)}")
    return contexto_str

def generar_respuesta(pregunta: str, contexto: str) -> str:
    """Env√≠a la pregunta y el contexto al LLM (gpt-oss-20b) para generar la respuesta."""
    
    # 1. Crea el Prompt Aumentado (System Prompt + Contexto + Pregunta)
    prompt_sistema = (
        "Eres un asistente de respuesta de preguntas que utiliza la informaci√≥n proporcionada en el Contexto para responder "
        "concisa y con precisi√≥n. Si la respuesta no est√° en el Contexto, indica claramente que no tienes suficiente informaci√≥n. "
        "NO inventes informaci√≥n."
    )
    
    prompt_usuario = (
        f"Contexto:\n---\n{contexto}\n---\n\n"
        f"Pregunta: {pregunta}"
    )

    # 2. Llama a la API de Chat (LM Studio emula el endpoint de Chat de OpenAI)
    print("ü§ñ Generando respuesta con el LLM...")
    try:
        response = cliente_api.chat.completions.create(
            model=NOMBRE_CHAT_MODELO,
            messages=[
                {"role": "system", "content": prompt_sistema},
                {"role": "user", "content": prompt_usuario}
            ],
            temperature=0.1 # Baja temperatura para respuestas m√°s f√°cticas
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"‚ùå ERROR al comunicarse con el LLM '{NOMBRE_CHAT_MODELO}'. Aseg√∫rate que est√© corriendo en LM Studio: {e}"


# ----------------------------------------
## 3. Bucle Principal de Preguntas
# ----------------------------------------
def bucle_preguntas():
    print("\n" + "="*50)
    print("      Sistema RAG Local Listo. ¬°Haz tu pregunta!")
    print(f"LLM: {NOMBRE_CHAT_MODELO} | Embeddings: {NOMBRE_EMBEDDING_MODELO}")
    print("Escribe 'salir' para terminar.")
    print("="*50 + "\n")

    while True:
        pregunta = input("‚ùì Tu pregunta: ").strip()
        
        if pregunta.lower() == 'salir':
            print("üëã ¬°Adi√≥s!")
            break
        if not pregunta:
            continue

        # 1. Vectorizar la pregunta
        vector_pre = vectorizar_pregunta(pregunta)
        if vector_pre is None:
            continue
        
        # 2. Buscar contexto en Faiss
        contexto = buscar_contexto(vector_pre, index, metadatos)
        
        # 3. Generar respuesta aumentada
        respuesta_final = generar_respuesta(pregunta, contexto)
        
        # 4. Mostrar resultado
        print("\n" + "-"*50)
        print("üí° Respuesta del LLM:")
        print(respuesta_final)
        print("-"*50 + "\n")

# Inicia el programa
bucle_preguntas()